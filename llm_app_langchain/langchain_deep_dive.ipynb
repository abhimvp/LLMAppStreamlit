{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a0ac3701",
   "metadata": {},
   "source": [
    "# Deep Dive into Langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1ac20f35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to learn Python_Langchain\n",
      "This is a test for Langchain\n"
     ]
    }
   ],
   "source": [
    "msg = \"Welcome to learn Python_Langchain\"\n",
    "print(msg)\n",
    "print(\"This is a test for Langchain\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5ef33ea8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: langchain\n",
      "Version: 0.3.23\n",
      "Summary: Building applications with LLMs through composability\n",
      "Home-page: \n",
      "Author: \n",
      "Author-email: \n",
      "License: MIT\n",
      "Location: c:\\Users\\abhis\\Desktop\\AIAgents\\LLMAppStreamlit\\env\\Lib\\site-packages\n",
      "Requires: langchain-core, langchain-text-splitters, langsmith, pydantic, PyYAML, requests, SQLAlchemy\n",
      "Required-by: \n",
      "---\n",
      "Name: langchain\n",
      "Version: 0.3.23\n",
      "Summary: Building applications with LLMs through composability\n",
      "Home-page: \n",
      "Author: \n",
      "Author-email: \n",
      "License: MIT\n",
      "Location: c:\\Users\\abhis\\Desktop\\AIAgents\\LLMAppStreamlit\\env\\Lib\\site-packages\n",
      "Requires: langchain-core, langchain-text-splitters, langsmith, pydantic, PyYAML, requests, SQLAlchemy\n",
      "Required-by: \n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Package(s) not found: #, check, installed, is, to, whether\n"
     ]
    }
   ],
   "source": [
    "pip show langchain # to check whether langchain is installed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ccee80d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: langchain-google-genai\n",
      "Version: 2.1.2\n",
      "Summary: An integration package connecting Google's genai package and LangChain\n",
      "Home-page: https://github.com/langchain-ai/langchain-google\n",
      "Author: \n",
      "Author-email: \n",
      "License: MIT\n",
      "Location: c:\\Users\\abhis\\Desktop\\AIAgents\\LLMAppStreamlit\\env\\Lib\\site-packages\n",
      "Requires: filetype, google-ai-generativelanguage, langchain-core, pydantic\n",
      "Required-by: \n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip show langchain-google-genai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08182837",
   "metadata": {},
   "source": [
    "### Python-dotenv\n",
    "Python-dotenv is a python module that allows you to specify environment variables as key value pairs in a .env file within your python project directory.It is a convinient and secure way to load and use environment variables in your application.We can have more than one API key and will save them all in .env.\n",
    "- In this project, we will have an API key for Google's GEMINI, another one for pinecone , Hugging Face and so on.\n",
    "- create an API Key to access google GEMINI models - [go here](https://aistudio.google.com/app/apikey) & copy the API KEY and save it to our .env file , which we will create.\n",
    "- let's load environment varaibles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cdc3d2e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv , find_dotenv\n",
    "# load the .env file\n",
    "load_dotenv(find_dotenv(),override=True)\n",
    "# to get the value of an environment variable\n",
    "# os.environ.get('GOOGLE_API_KEY') - display the key"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62c29215",
   "metadata": {},
   "source": [
    "- We know that langchain provides a standard interface for interacting with various LLMs, including openAI's GPT models , Google's GEMINI, Meta's LLAMA ..etc\n",
    "- In Langchain terminology these are called LLM providers.\n",
    "- `Let's see how to invoke the models like Google GEMINI.`\n",
    "- The models expose an interface where chat messages or conversations serve as inputs & outputs\n",
    "- refer this [documentation](https://ai.google.dev/gemini-api/docs) to understand more on what to use and how to use like chat-completions.\n",
    "- [Multi-turn conversation](https://ai.google.dev/gemini-api/docs/text-generation#multi-turn-conversations) & [system instructions](https://ai.google.dev/gemini-api/docs/text-generation#system-instructions) - System instructions let you steer the behavior of a model based on your specific use case. When you provide system instructions, you give the model additional context to help it understand the task and generate more customized responses. The model should adhere to the system instructions over the full interaction with the user, enabling you to specify product-level behavior separate from the prompts provided by end users.\n",
    "- from langchain we use [messages](https://python.langchain.com/api_reference/core/messages.html) - Messages are objects used in prompts and chat conversations.\n",
    "- like below:\n",
    "```py\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "messages = [\n",
    "    SystemMessage(\n",
    "        content=\"You are a helpful assistant! Your name is Bob.\"\n",
    "    ),\n",
    "    HumanMessage(\n",
    "        content=\"What is your name?\"\n",
    "    )\n",
    "]\n",
    "# Instantiate a chat model and invoke it with the messages\n",
    "model = ...\n",
    "print(model.invoke(messages))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1c93ff60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Virat Kohli's batting style is characterized by his aggressive, yet technically sound approach, combining powerful strokeplay with exceptional running between the wickets and a relentless pursuit of excellence.\n"
     ]
    }
   ],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash\")\n",
    "output = llm.invoke(\"explain virat kohli's batting style in one sentence\")\n",
    "print(output.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "47710000",
   "metadata": {},
   "outputs": [],
   "source": [
    "# help(ChatGoogleGenerativeAI) \n",
    "# - gives whole documentation stuff on this library \n",
    "# like Help on class ChatGoogleGenerativeAI in module langchain_google_genai.chat_models: ....etc\n",
    "# we can see the default models and temparature and other parameters in the documentation is set to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "078848e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='The capital of India is New Delhi.', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.0-flash', 'safety_ratings': []}, id='run-94872e41-a6b2-4a51-b099-aaf00334c829-0', usage_metadata={'input_tokens': 12, 'output_tokens': 9, 'total_tokens': 21, 'input_token_details': {'cache_read': 0}})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage,SystemMessage,AIMessage\n",
    "messages = [\n",
    "    SystemMessage(content=\"You are a helpful assistant\"),\n",
    "    HumanMessage(content=\"What is the capital of India?\")\n",
    "]\n",
    "llm.invoke(messages)\n",
    "# print(assistant_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90a4e089",
   "metadata": {},
   "source": [
    "- so above we can see that we're passing how the LLM to behave by using Messages(AI,System,Human) to let llm understand what it needs to do and respond accordingly.\n",
    "- [SystemMessage](https://python.langchain.com/api_reference/core/messages/langchain_core.messages.system.SystemMessage.html) - Message for priming AI behavior. The system message is usually passed in as the first of a sequence of input messages.`Pass in content as positional arg.`\n",
    "- [HumanMessage](https://python.langchain.com/api_reference/core/messages/langchain_core.messages.human.HumanMessage.html) - Message from a human. HumanMessages are messages that are passed in from a human to the model.\n",
    "- [AIMessage](https://python.langchain.com/api_reference/core/messages/langchain_core.messages.ai.AIMessage.html) - Message from an AI.`AIMessage is returned from a chat model as a response to a prompt`.This message represents the output of the model and consists of both the raw output as returned by the model together standardized fields (e.g., tool calls, usage metadata) added by the LangChain framework.\n",
    "- this is how the `actual` assistant_response from above looks like which returns the AIMessage.\n",
    "```py\n",
    "AIMessage(content='The capital of India is New Delhi.', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.0-flash', 'safety_ratings': []}, id='run-94872e41-a6b2-4a51-b099-aaf00334c829-0', usage_metadata={'input_tokens': 12, 'output_tokens': 9, 'total_tokens': 21, 'input_token_details': {'cache_read': 0}})\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9056318",
   "metadata": {},
   "source": [
    "### Caching in LangChain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "685399ac",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
